{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of carrac_kamil.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "87FQJr8ALCWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.endpoint.com/blog/2018/08/29/self-driving-toy-car-using-the-a3c-algorithm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs-D3CfnLCWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import collections\n",
        "from collections import deque\n",
        "import multiprocessing as mp\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeCSGYdwLo9U",
        "colab_type": "code",
        "outputId": "f42d6e5b-8665-40f9-d684-8af146337f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install gym[box2d]\n",
        "!pip install gym[all]\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "#!pip install time\n",
        "import time\n",
        "import itertools\n",
        "import traceback\n",
        "import logging\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1\n",
        "logger=logging.getLogger('foo')    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.18.4)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.4.1)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.3.8)\n",
            "Collecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "  Using cached https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.11.0)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.17)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.0)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ye6vv5bl/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ye6vv5bl/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-27k4et8j/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.0.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (19.3.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.34.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7GiByJQLCWc",
        "colab_type": "text"
      },
      "source": [
        "uniform = [0.25, 0.25, 0.25, 0.25]\n",
        "more confident = [0.5, 0.25, 0.15, 0.10]\n",
        "over confident = [0.95, 0.01, 0.01, 0.03]\n",
        "super over confident = [0.99, 0.003, 0.004, 0.003]\n",
        "\n",
        "y(x) = x*log(x, 10)\n",
        "\n",
        "entropy(dist) = -sum(map(y, dist))\n",
        "\n",
        "entropy (uniform) => 0.6021\n",
        "entropy (more confident) => 0.5246\n",
        "entropy (over confident) => 0.1068\n",
        "entropy (super over confident) => 0.0291"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbCxJX7SLCWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Agent, self).__init__(**kwargs)\n",
        "\n",
        "        self.init_args = kwargs\n",
        "\n",
        "        self.h = torch.zeros(1, 256)\n",
        "\n",
        "        self.norm1 = nn.BatchNorm2d(4)\n",
        "        self.norm2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(4, 32, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "\n",
        "        self.gru = nn.GRUCell(1152, 256)\n",
        "        self.policy = nn.Linear(256, 4)\n",
        "        self.value = nn.Linear(256, 1)\n",
        "\n",
        "        self.prelu1 = nn.PReLU()\n",
        "        self.prelu2 = nn.PReLU()\n",
        "        self.prelu3 = nn.PReLU()\n",
        "        self.prelu4 = nn.PReLU()\n",
        "\n",
        "        nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        nn.init.constant_(self.conv1.bias, 0.01)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        nn.init.constant_(self.conv2.bias, 0.01)\n",
        "        nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        nn.init.constant_(self.conv3.bias, 0.01)\n",
        "        nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        nn.init.constant_(self.conv4.bias, 0.01)\n",
        "        nn.init.constant_(self.gru.bias_ih, 0)\n",
        "        nn.init.constant_(self.gru.bias_hh, 0)\n",
        "        nn.init.xavier_uniform_(self.policy.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        nn.init.constant_(self.policy.bias, 0.01)\n",
        "        nn.init.xavier_uniform_(self.value.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        nn.init.constant_(self.value.bias, 0.01)\n",
        "\n",
        "        self.train()\n",
        "\n",
        "    def reset(self):\n",
        "        self.h = torch.zeros(1, 256)\n",
        "\n",
        "    def clone(self, num=1):\n",
        "        return [ self.clone_one() for _ in range(num) ]\n",
        "\n",
        "    def clone_one(self):\n",
        "        return Agent(**self.init_args)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = state.view(1, 4, 96, 96)\n",
        "        state = self.norm1(state)\n",
        "\n",
        "        data = self.prelu1(self.conv1(state))\n",
        "        data = self.prelu2(self.conv2(data))\n",
        "        data = self.prelu3(self.conv3(data))\n",
        "        data = self.prelu4(self.conv4(data))\n",
        "\n",
        "        data = self.norm2(data)\n",
        "        data = data.view(1, -1)\n",
        "\n",
        "        h = self.gru(data, self.h)\n",
        "        self.h = h.detach()\n",
        "\n",
        "        pre_policy = h.view(-1)\n",
        "\n",
        "        policy = F.softmax(self.policy(pre_policy))\n",
        "        value = self.value(pre_policy)\n",
        "\n",
        "        return policy, value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SMz1vyYLCWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Runner:\n",
        "    def __init__(self, agent, ix, train = True, **kwargs):\n",
        "        self.agent = agent\n",
        "        self.train = train\n",
        "        self.ix = ix\n",
        "        self.reset = False\n",
        "        self.states = []\n",
        "\n",
        "        # each runner has its own environment:\n",
        "        self.env = gym.make('CarRacing-v0')\n",
        "\n",
        "    def get_value(self):\n",
        "        \"\"\"\n",
        "        Returns just the current state's value.\n",
        "        This is used when approximating the R.\n",
        "        If the last step was\n",
        "        not terminal, then we're substituting the \"r\"\n",
        "        with V(s) - hence, we need a way to just\n",
        "        get that V(s) without moving forward yet.\n",
        "        \"\"\"\n",
        "        _input = self.preprocess(self.states)\n",
        "        _, _, _, value = self.decide(_input)\n",
        "        return value\n",
        "\n",
        "    def run_episode(self, yield_every = 10, do_render = False):\n",
        "        \"\"\"\n",
        "        The episode runner written in the generator style.\n",
        "        This is meant to be used in a \"for (...) in run_episode(...):\" manner.\n",
        "        Each value generated is a tuple of:\n",
        "        step_ix: the current \"step\" number\n",
        "        rewards: the list of rewards as received from the environment (without discounting yet)\n",
        "        values: the list of V(s) values, as predicted by the \"critic\"\n",
        "        policies: the list of policies as received from the \"actor\"\n",
        "        actions: the list of actions as sampled based on policies\n",
        "        terminal: whether we're in a \"terminal\" state\n",
        "        \"\"\"\n",
        "        self.reset = False\n",
        "        step_ix = 0\n",
        "\n",
        "        rewards, values, policies, actions = [[], [], [], []]\n",
        "\n",
        "        self.env.reset()\n",
        "\n",
        "        # we're going to feed the last 4 frames to the neural network that acts as the \"actor-critic\" duo. We'll use the \"deque\" to efficiently drop too old frames always keeping its length at 4:\n",
        "        states = deque([ ])\n",
        "\n",
        "        # we're pre-populating the states deque by taking first 4 steps as \"full throttle forward\":\n",
        "        while len(states) < 4:\n",
        "            _, r, _, _ = self.env.step([0.0, 1.0, 0.0])\n",
        "            state = self.env.render(mode='rgb_array')\n",
        "            states.append(state)\n",
        "            logger.info('Init reward ' + str(r) )\n",
        "\n",
        "        # we need to repeat the following as long as the game is not over yet:\n",
        "        while True:\n",
        "            # the frames need to be preprocessed (I'm explaining the reasons later in the article)\n",
        "            _input = self.preprocess(states)\n",
        "\n",
        "            # asking the neural network for the policy and value predictions:\n",
        "            action, action_ix, policy, value = self.decide(_input, step_ix)\n",
        "\n",
        "            # taking the step and receiving the reward along with info if the game is over:\n",
        "            _, reward, terminal, _ = self.env.step(action)\n",
        "\n",
        "            # explicitly rendering the scene (again, this will be explained later)\n",
        "            state = self.env.render(mode='rgb_array')\n",
        "\n",
        "            # update the last 4 states deque:\n",
        "            states.append(state)\n",
        "            while len(states) > 4:\n",
        "                states.popleft()\n",
        "\n",
        "            # if we've been asked to render into the window (e. g. to capture the video):\n",
        "            if do_render:\n",
        "                self.env.render()\n",
        "\n",
        "            self.states = states\n",
        "            step_ix += 1\n",
        "\n",
        "            rewards.append(reward)\n",
        "            values.append(value)\n",
        "            policies.append(policy)\n",
        "            actions.append(action_ix)\n",
        "\n",
        "            # periodically save the state's screenshot along with the numerical values in an easy to read way:\n",
        "            if self.ix == 2 and step_ix % 200 == 0:\n",
        "                fname = './screens/car-racing/screen-' + str(step_ix) + '-' + str(int(time.time())) + '.jpg'\n",
        "                im = Image.fromarray(state)\n",
        "                im.save(fname)\n",
        "                state.tofile(fname + '.txt', sep=\" \")\n",
        "                _input.numpy().tofile(fname + '.input.txt', sep=\" \")\n",
        "\n",
        "            # if it's game over or we hit the \"yield every\" value, yield the values from this generator:\n",
        "            if terminal or step_ix % yield_every == 0:\n",
        "                yield step_ix, rewards, values, policies, actions, terminal\n",
        "                rewards, values, policies, actions = [[], [], [], []]\n",
        "\n",
        "            # following is a very tacky way to allow external using code to mark that it wants us to reset the environment, finishing the episode prematurely. (this would be hugely refactored in the production code but for the sake of playing with the algorithm itself, it's good enough):\n",
        "            if self.reset:\n",
        "                self.reset = False\n",
        "                self.agent.reset()\n",
        "                states = deque([ ])\n",
        "                self.states = deque([ ])\n",
        "                return\n",
        "\n",
        "            if terminal:\n",
        "                self.agent.reset()\n",
        "                states = deque([ ])\n",
        "                return\n",
        "    def ask_reset(self):\n",
        "        self.reset = True\n",
        "\n",
        "    def preprocess(self, states):\n",
        "        return torch.stack([ torch.tensor(self.preprocess_one(image_data), dtype=torch.float32) for image_data in states ])\n",
        "\n",
        "    def preprocess_one(self, image):\n",
        "        \"\"\"\n",
        "        Scales the rendered image and makes it grayscale\n",
        "        \"\"\"\n",
        "        return rescale(rgb2gray(image), (0.24, 0.16), anti_aliasing=False, mode='edge', multichannel=False)\n",
        "\n",
        "    def choose_action(self, policy, step_ix):\n",
        "        \"\"\"\n",
        "        Chooses an action to take based on the policy and whether we're in the training mode or not. During training, it samples based on the probability values in the policy. During the evaluation, it takes the most probable action in a greedy way.\n",
        "        \"\"\"\n",
        "        policies = [[-0.8, 0.0, 0.0], [0.8, 0.0, 0], [0.0, 0.1, 0.0], [0.0, 0.0, 0.6]]\n",
        "\n",
        "        if self.train:\n",
        "            action_ix = np.random.choice(4, 1, p=torch.tensor(policy).detach().numpy())[0]\n",
        "        else:\n",
        "            action_ix = np.argmax(torch.tensor(policy).detach().numpy())\n",
        "\n",
        "        logger.info('Step ' + str(step_ix) + ' Runner ' + str(self.ix) + ' Action ix: ' + str(action_ix) + ' From: ' + str(policy))\n",
        "\n",
        "        return np.array(policies[action_ix], dtype=np.float32), action_ix\n",
        "\n",
        "    def decide(self, state, step_ix = 999):\n",
        "        policy, value = self.agent(state)\n",
        "\n",
        "        action, action_ix = self.choose_action(policy, step_ix)\n",
        "\n",
        "        return action, action_ix, policy, value\n",
        "\n",
        "    def load_state_dict(self, state):\n",
        "        \"\"\"\n",
        "        As we'll have multiple \"worker\" runners, they will need to be able to sync their agents' weights with the main agent.\n",
        "        This function loads the weights into this runner's agent.\n",
        "        \"\"\"\n",
        "        self.agent.load_state_dict(state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzWom5fULCWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, gamma, agent, window = 15, workers = 8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.agent = agent\n",
        "        self.window = window\n",
        "        self.gamma = gamma\n",
        "        self.optimizer = optim.Adam(self.agent.parameters(), lr=1e-4)\n",
        "        self.workers = workers\n",
        "\n",
        "        # even though we're loading the weights into worker agents explicitly, I found that still without sharing the weights as following, the algorithm was not converging:\n",
        "        self.agent.share_memory()\n",
        "\n",
        "    def fit(self, episodes = 1000):\n",
        "#            \"\"\"\n",
        "#            The higher level method for training the agents.\n",
        "#            It called into the lower level \"train\" which orchestrates the process itself.\n",
        "#            \"\"\"\n",
        "        last_update = 0\n",
        "        updates = dict()\n",
        "\n",
        "        for ix in range(1, self.workers + 1):\n",
        "            updates[ ix ] = { 'episode': 0, 'step': 0, 'rewards': deque(), 'losses': deque(), 'points': 0, 'mean_reward': 0, 'mean_loss': 0 }\n",
        "\n",
        "        for update in self.train(episodes):\n",
        "            now = time.time()\n",
        "\n",
        "            # you could do something useful here with the updates dict.\n",
        "            # I've opted out as I'm using logging anyways and got more value in just watching the log file, grepping for the desired values\n",
        "\n",
        "            # save the current model's weights every minute:\n",
        "            if now - last_update > 60:\n",
        "                torch.save(self.agent.state_dict(), './checkpoints/car-racing/' + str(int(now)) + '-.pytorch')\n",
        "                last_update = now\n",
        "\n",
        "    def train(self, episodes = 1000):\n",
        "        \"\"\"\n",
        "        Lower level training orchestration method. Written in the generator style. Intended to be used with \"for update in train(...):\"\n",
        "        \"\"\"\n",
        "\n",
        "        # create the requested number of background agents and runners:\n",
        "        worker_agents = self.agent.clone(num = self.workers)\n",
        "        runners = [ Runner(agent=agent, ix = ix + 1, train = True) for ix, agent in enumerate(worker_agents) ]\n",
        "\n",
        "        # we're going to communicate the workers' updates via the thread safe queue:\n",
        "        queue = mp.SimpleQueue()\n",
        "\n",
        "        # if we've not been given a number of episodes: assume the process is going to be interrupted with the keyboard interrupt once the user (us) decides so:\n",
        "        if episodes is None:\n",
        "            print('Starting out an infinite training process')\n",
        "\n",
        "        # create the actual background processes, making their entry be the train_one method:\n",
        "        processes = [ mp.Process(target=self.train_one, args=(runners[ix - 1], queue, episodes, ix)) for ix in range(1, self.workers + 1) ]\n",
        "\n",
        "        # run those processes:\n",
        "        for process in processes:\n",
        "            process.start()\n",
        "\n",
        "        try:\n",
        "            # what follows is a rather naive implementation of listening to workers updates. it works though for our purposes:\n",
        "            while any([ process.is_alive() for process in processes ]):\n",
        "                results = queue.get()\n",
        "                yield results\n",
        "        except Exception as e:\n",
        "            logger.error(str(e))\n",
        "\n",
        "    def train_one(self, runner, queue, episodes = 1000, ix = 1):\n",
        "        \"\"\"\n",
        "        Orchestrate the training for a single worker runner and agent. This is intended to run in its own background process.\n",
        "        \"\"\"\n",
        "\n",
        "        # possibly naive way of trying to de-correlate the weight updates further (I have no hard evidence to prove if it works, other than my subjective observation):\n",
        "        time.sleep(ix)\n",
        "\n",
        "        try:\n",
        "            # we are going to request the episode be reset whenever our agent scores lower than its max points. the same will happen if the agent scores total of -10 points:\n",
        "            max_points = 0\n",
        "            max_eval_points = 0\n",
        "            min_points = 0\n",
        "            max_episode = 0\n",
        "\n",
        "            for episode_ix in itertools.count(start=0, step=1):\n",
        "\n",
        "                if episodes is not None and episode_ix >= episodes:\n",
        "                    return\n",
        "\n",
        "                max_episode_points = 0\n",
        "                points = 0\n",
        "\n",
        "                # load up the newest weights every new episode:\n",
        "                runner.load_state_dict(self.agent.state_dict())\n",
        "\n",
        "                # every 5 episodes lets evaluate the weights we've learned so far by recording the run of the car using the greedy strategy:\n",
        "                if ix == 1 and episode_ix % 5 == 0:\n",
        "                    eval_points = self.record_greedy(episode_ix)\n",
        "\n",
        "                    if eval_points > max_eval_points:\n",
        "                        torch.save(runner.agent.state_dict(), './checkpoints/car-racing/' + str(eval_points) + '-eval-points.pytorch')\n",
        "                        max_eval_points = eval_points\n",
        "\n",
        "                # each n-step window, compute the gradients and apply\n",
        "                # also: decide if we shouldn't restart the episode if we don't want to explore too much of the not-useful state space:\n",
        "                for step, rewards, values, policies, action_ixs, terminal in runner.run_episode(yield_every=self.window):\n",
        "                    points += sum(rewards)\n",
        "\n",
        "                    if ix == 1 and points > max_points:\n",
        "                        torch.save(runner.agent.state_dict(), './checkpoints/car-racing/' + str(points) + '-points.pytorch')\n",
        "                        max_points = points\n",
        "\n",
        "                    if ix == 1 and episode_ix > max_episode:\n",
        "                        torch.save(runner.agent.state_dict(), './checkpoints/car-racing/' + str(episode_ix) + '-episode.pytorch')\n",
        "                        max_episode = episode_ix\n",
        "\n",
        "                    if points < -10 or (max_episode_points > min_points and points < min_points):\n",
        "                        terminal = True\n",
        "                        max_episode_points = 0\n",
        "                        point = 0\n",
        "                        runner.ask_reset()\n",
        "\n",
        "                    if terminal:\n",
        "                        logger.info('TERMINAL for ' + str(ix) + ' at step ' + str(step) + ' with total points ' + str(points) + ' max: ' + str(max_episode_points) )\n",
        "\n",
        "                    # if we're learning, then compute and apply the gradients and load the newest weights:\n",
        "                    if runner.train:\n",
        "                        loss = self.apply_gradients(policies, action_ixs, rewards, values, terminal, runner)\n",
        "                        runner.load_state_dict(self.agent.state_dict())\n",
        "\n",
        "                    max_episode_points = max(max_episode_points, points)\n",
        "                    min_points = max(min_points, points)\n",
        "\n",
        "                    # communicate the gathered values to the main process:\n",
        "                    queue.put((ix, episode_ix, step, rewards, loss, points, terminal))\n",
        "\n",
        "        except Exception as e:\n",
        "            string = traceback.format_exc()\n",
        "            logger.error(str(e) + ' → ' + string)\n",
        "            queue.put((ix, -1, -1, [-1], -1, str(e) + '<br />' + string, True))\n",
        "\n",
        "    def record_greedy(self, episode_ix):\n",
        "        \"\"\"\n",
        "        Records the video of the \"greedy\" run based on the current weights.\n",
        "        \"\"\"\n",
        "        directory = './videos/car-racing/episode-' + str(episode_ix) + '-' + str(int(time.time()))\n",
        "        player = Player(agent=self.agent, directory=directory, train=False)\n",
        "        points = player.play()\n",
        "        logger.info('Evaluation at episode ' + str(episode_ix) + ': ' + str(points) + ' points (' + directory + ')')\n",
        "        return points\n",
        "\n",
        "    def apply_gradients(self, policies, actions, rewards, values, terminal, runner):\n",
        "        worker_agent = runner.agent\n",
        "        actions_one_hot = torch.tensor([[ int(i == action) for i in range(4) ] for action in actions], dtype=torch.float32)\n",
        "\n",
        "        policies = torch.stack(policies)\n",
        "        values = torch.cat(values)\n",
        "        values_nograd = torch.zeros_like(values.detach(), requires_grad=False)\n",
        "        values_nograd.copy_(values)\n",
        "\n",
        "        discounted_rewards = self.discount_rewards(runner, rewards, values_nograd[-1], terminal)\n",
        "        advantages = discounted_rewards - values_nograd\n",
        "\n",
        "        logger.info('Runner ' + str(runner.ix) + 'Rewards: ' + str(rewards))\n",
        "        logger.info('Runner ' + str(runner.ix) + 'Discounted Rewards: ' + str(discounted_rewards.numpy()))\n",
        "\n",
        "        log_policies = torch.log(0.00000001 + policies)\n",
        "\n",
        "        one_log_policies = torch.sum(log_policies * actions_one_hot, dim=1)\n",
        "\n",
        "        entropy = torch.sum(policies * -log_policies)\n",
        "\n",
        "        policy_loss = -torch.mean(one_log_policies * advantages)\n",
        "\n",
        "        value_loss = F.mse_loss(values, discounted_rewards)\n",
        "\n",
        "        value_loss_nograd = torch.zeros_like(value_loss)\n",
        "        value_loss_nograd.copy_(value_loss)\n",
        "\n",
        "        policy_loss_nograd = torch.zeros_like(policy_loss)\n",
        "        policy_loss_nograd.copy_(policy_loss)\n",
        "\n",
        "        logger.info('Value Loss: ' + str(float(value_loss_nograd)) + ' Policy Loss: ' + str(float(policy_loss_nograd)))\n",
        "\n",
        "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
        "        self.agent.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(worker_agent.parameters(), 40)\n",
        "\n",
        "        # the following step is crucial. at this point, all the info about the gradients reside in the worker agent's memory. We need to \"move\" those gradients into the main agent's memory:\n",
        "        self.share_gradients(worker_agent)\n",
        "\n",
        "        # update the weights with the computed gradients:\n",
        "        self.optimizer.step()\n",
        "\n",
        "        worker_agent.zero_grad()\n",
        "        return float(loss.detach())\n",
        "\n",
        "    def share_gradients(self, worker_agent):\n",
        "        for param, shared_param in zip(worker_agent.parameters(), self.agent.parameters()):\n",
        "            if shared_param.grad is not None:\n",
        "                return\n",
        "            shared_param._grad = param.grad\n",
        "\n",
        "    def clip_reward(self, reward):\n",
        "        \"\"\"\n",
        "        Clips the rewards into the <-3, 3> range preventing too big of the gradients variance.\n",
        "        \"\"\"\n",
        "        return max(min(reward, 3), -3)\n",
        "\n",
        "    def discount_rewards(self, runner, rewards, last_value, terminal):\n",
        "        discounted_rewards = [0 for _ in rewards]\n",
        "        loop_rewards = [ self.clip_reward(reward) for reward in rewards ]\n",
        "\n",
        "        if terminal:\n",
        "            loop_rewards.append(0)\n",
        "        else:\n",
        "            loop_rewards.append(runner.get_value())\n",
        "\n",
        "        for main_ix in range(len(discounted_rewards) - 1, -1, -1):\n",
        "            for inside_ix in range(len(loop_rewards) - 1, -1, -1):\n",
        "                if inside_ix >= main_ix:\n",
        "                    reward = loop_rewards[inside_ix]\n",
        "                    discounted_rewards[main_ix] += self.gamma**(inside_ix - main_ix) * reward\n",
        "\n",
        "        return torch.tensor(discounted_rewards)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc7yM5OULCWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Player(Runner):\n",
        "    def __init__(self, directory, **kwargs):\n",
        "        super().__init__(ix=999, **kwargs)\n",
        "\n",
        "        self.env = Monitor(self.env, directory)\n",
        "\n",
        "    def play(self):\n",
        "        points = 0\n",
        "        for step, rewards, values, policies, actions, terminal in self.run_episode(yield_every = 1, do_render = True):\n",
        "            points += sum(rewards)\n",
        "        self.env.close()\n",
        "        return points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ngOq3MGLCWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEjCDYBaLCWm",
        "colab_type": "code",
        "outputId": "6d2fc569-a623-428c-9451-6035b614b678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    agent = Agent()\n",
        "\n",
        "    trainer = Trainer(gamma = 0.99, agent = agent)\n",
        "    trainer.fit(episodes=None)\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting out an infinite training process\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "name 'Monitor' is not defined → Traceback (most recent call last):\n",
            "  File \"<ipython-input-14-f2ceb8b559a8>\", line 95, in train_one\n",
            "    eval_points = self.record_greedy(episode_ix)\n",
            "  File \"<ipython-input-14-f2ceb8b559a8>\", line 144, in record_greedy\n",
            "    player = Player(agent=self.agent, directory=directory, train=False)\n",
            "  File \"<ipython-input-15-45a03fe63232>\", line 5, in __init__\n",
            "    self.env = Monitor(self.env, directory)\n",
            "NameError: name 'Monitor' is not defined\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-391110e4753f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f2ceb8b559a8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# save the current model's weights every minute:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_update\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./checkpoints/car-racing/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-.pytorch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mlast_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints/car-racing/1589285355-.pytorch'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNM9lw3XLCWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}