{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.endpoint.com/blog/2018/08/29/self-driving-toy-car-using-the-a3c-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import collections\n",
    "from collections import deque\n",
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uniform = [0.25, 0.25, 0.25, 0.25]\n",
    "more confident = [0.5, 0.25, 0.15, 0.10]\n",
    "over confident = [0.95, 0.01, 0.01, 0.03]\n",
    "super over confident = [0.99, 0.003, 0.004, 0.003]\n",
    "\n",
    "y(x) = x*log(x, 10)\n",
    "\n",
    "entropy(dist) = -sum(map(y, dist))\n",
    "\n",
    "entropy (uniform) => 0.6021\n",
    "entropy (more confident) => 0.5246\n",
    "entropy (over confident) => 0.1068\n",
    "entropy (super over confident) => 0.0291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Agent, self).__init__(**kwargs)\n",
    "\n",
    "        self.init_args = kwargs\n",
    "\n",
    "        self.h = torch.zeros(1, 256)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(4)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.gru = nn.GRUCell(1152, 256)\n",
    "        self.policy = nn.Linear(256, 4)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        self.prelu3 = nn.PReLU()\n",
    "        self.prelu4 = nn.PReLU()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "        nn.init.constant_(self.conv1.bias, 0.01)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "        nn.init.constant_(self.conv2.bias, 0.01)\n",
    "        nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "        nn.init.constant_(self.conv3.bias, 0.01)\n",
    "        nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "        nn.init.constant_(self.conv4.bias, 0.01)\n",
    "        nn.init.constant_(self.gru.bias_ih, 0)\n",
    "        nn.init.constant_(self.gru.bias_hh, 0)\n",
    "        nn.init.xavier_uniform_(self.policy.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "        nn.init.constant_(self.policy.bias, 0.01)\n",
    "        nn.init.xavier_uniform_(self.value.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "        nn.init.constant_(self.value.bias, 0.01)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def reset(self):\n",
    "        self.h = torch.zeros(1, 256)\n",
    "\n",
    "    def clone(self, num=1):\n",
    "        return [ self.clone_one() for _ in range(num) ]\n",
    "\n",
    "    def clone_one(self):\n",
    "        return Agent(**self.init_args)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.view(1, 4, 96, 96)\n",
    "        state = self.norm1(state)\n",
    "\n",
    "        data = self.prelu1(self.conv1(state))\n",
    "        data = self.prelu2(self.conv2(data))\n",
    "        data = self.prelu3(self.conv3(data))\n",
    "        data = self.prelu4(self.conv4(data))\n",
    "\n",
    "        data = self.norm2(data)\n",
    "        data = data.view(1, -1)\n",
    "\n",
    "        h = self.gru(data, self.h)\n",
    "        self.h = h.detach()\n",
    "\n",
    "        pre_policy = h.view(-1)\n",
    "\n",
    "        policy = F.softmax(self.policy(pre_policy))\n",
    "        value = self.value(pre_policy)\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self, agent, ix, train = True, **kwargs):\n",
    "        self.agent = agent\n",
    "        self.train = train\n",
    "        self.ix = ix\n",
    "        self.reset = False\n",
    "        self.states = []\n",
    "\n",
    "        # each runner has its own environment:\n",
    "        self.env = gym.make('CarRacing-v0')\n",
    "\n",
    "    def get_value(self):\n",
    "        \"\"\"\n",
    "        Returns just the current state's value.\n",
    "        This is used when approximating the R.\n",
    "        If the last step was\n",
    "        not terminal, then we're substituting the \"r\"\n",
    "        with V(s) - hence, we need a way to just\n",
    "        get that V(s) without moving forward yet.\n",
    "        \"\"\"\n",
    "        _input = self.preprocess(self.states)\n",
    "        _, _, _, value = self.decide(_input)\n",
    "        return value\n",
    "\n",
    "    def run_episode(self, yield_every = 10, do_render = False):\n",
    "        \"\"\"\n",
    "        The episode runner written in the generator style.\n",
    "        This is meant to be used in a \"for (...) in run_episode(...):\" manner.\n",
    "        Each value generated is a tuple of:\n",
    "        step_ix: the current \"step\" number\n",
    "        rewards: the list of rewards as received from the environment (without discounting yet)\n",
    "        values: the list of V(s) values, as predicted by the \"critic\"\n",
    "        policies: the list of policies as received from the \"actor\"\n",
    "        actions: the list of actions as sampled based on policies\n",
    "        terminal: whether we're in a \"terminal\" state\n",
    "        \"\"\"\n",
    "        self.reset = False\n",
    "        step_ix = 0\n",
    "\n",
    "        rewards, values, policies, actions = [[], [], [], []]\n",
    "\n",
    "        self.env.reset()\n",
    "\n",
    "        # we're going to feed the last 4 frames to the neural network that acts as the \"actor-critic\" duo. We'll use the \"deque\" to efficiently drop too old frames always keeping its length at 4:\n",
    "        states = deque([ ])\n",
    "\n",
    "        # we're pre-populating the states deque by taking first 4 steps as \"full throttle forward\":\n",
    "        while len(states) < 4:\n",
    "            _, r, _, _ = self.env.step([0.0, 1.0, 0.0])\n",
    "            state = self.env.render(mode='rgb_array')\n",
    "            states.append(state)\n",
    "            logger.info('Init reward ' + str(r) )\n",
    "\n",
    "        # we need to repeat the following as long as the game is not over yet:\n",
    "        while True:\n",
    "            # the frames need to be preprocessed (I'm explaining the reasons later in the article)\n",
    "            _input = self.preprocess(states)\n",
    "\n",
    "            # asking the neural network for the policy and value predictions:\n",
    "            action, action_ix, policy, value = self.decide(_input, step_ix)\n",
    "\n",
    "            # taking the step and receiving the reward along with info if the game is over:\n",
    "            _, reward, terminal, _ = self.env.step(action)\n",
    "\n",
    "            # explicitly rendering the scene (again, this will be explained later)\n",
    "            state = self.env.render(mode='rgb_array')\n",
    "\n",
    "            # update the last 4 states deque:\n",
    "            states.append(state)\n",
    "            while len(states) > 4:\n",
    "                states.popleft()\n",
    "\n",
    "            # if we've been asked to render into the window (e. g. to capture the video):\n",
    "            if do_render:\n",
    "                self.env.render()\n",
    "\n",
    "            self.states = states\n",
    "            step_ix += 1\n",
    "\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            policies.append(policy)\n",
    "            actions.append(action_ix)\n",
    "\n",
    "            # periodically save the state's screenshot along with the numerical values in an easy to read way:\n",
    "            if self.ix == 2 and step_ix % 200 == 0:\n",
    "                fname = '.'+os.sep+'screens'+os.sep+'car-racing'+os.sep+'screen-' + str(step_ix) + '-' + str(int(time.time())) + '.jpg'\n",
    "                im = Image.fromarray(state)\n",
    "                im.save(fname)\n",
    "                state.tofile(fname + '.txt', sep=\" \")\n",
    "                _input.numpy().tofile(fname + '.input.txt', sep=\" \")\n",
    "\n",
    "            # if it's game over or we hit the \"yield every\" value, yield the values from this generator:\n",
    "            if terminal or step_ix % yield_every == 0:\n",
    "                yield step_ix, rewards, values, policies, actions, terminal\n",
    "                rewards, values, policies, actions = [[], [], [], []]\n",
    "\n",
    "            # following is a very tacky way to allow external using code to mark that it wants us to reset the environment, finishing the episode prematurely. (this would be hugely refactored in the production code but for the sake of playing with the algorithm itself, it's good enough):\n",
    "            if self.reset:\n",
    "                self.reset = False\n",
    "                self.agent.reset()\n",
    "                states = deque([ ])\n",
    "                self.states = deque([ ])\n",
    "                return\n",
    "\n",
    "            if terminal:\n",
    "                self.agent.reset()\n",
    "                states = deque([ ])\n",
    "                return\n",
    "    def ask_reset(self):\n",
    "        self.reset = True\n",
    "\n",
    "    def preprocess(self, states):\n",
    "        return torch.stack([ torch.tensor(self.preprocess_one(image_data), dtype=torch.float32) for image_data in states ])\n",
    "\n",
    "    def preprocess_one(self, image):\n",
    "        \"\"\"\n",
    "        Scales the rendered image and makes it grayscale\n",
    "        \"\"\"\n",
    "        return rescale(rgb2gray(image), (0.24, 0.16), anti_aliasing=False, mode='edge', multichannel=False)\n",
    "\n",
    "    def choose_action(self, policy, step_ix):\n",
    "        \"\"\"\n",
    "        Chooses an action to take based on the policy and whether we're in the training mode or not. During training, it samples based on the probability values in the policy. During the evaluation, it takes the most probable action in a greedy way.\n",
    "        \"\"\"\n",
    "        policies = [[-0.8, 0.0, 0.0], [0.8, 0.0, 0], [0.0, 0.1, 0.0], [0.0, 0.0, 0.6]]\n",
    "\n",
    "        if self.train:\n",
    "            action_ix = np.random.choice(4, 1, p=torch.tensor(policy).detach().numpy())[0]\n",
    "        else:\n",
    "            action_ix = np.argmax(torch.tensor(policy).detach().numpy())\n",
    "\n",
    "        logger.info('Step ' + str(step_ix) + ' Runner ' + str(self.ix) + ' Action ix: ' + str(action_ix) + ' From: ' + str(policy))\n",
    "\n",
    "        return np.array(policies[action_ix], dtype=np.float32), action_ix\n",
    "\n",
    "    def decide(self, state, step_ix = 999):\n",
    "        policy, value = self.agent(state)\n",
    "\n",
    "        action, action_ix = self.choose_action(policy, step_ix)\n",
    "\n",
    "        return action, action_ix, policy, value\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        \"\"\"\n",
    "        As we'll have multiple \"worker\" runners, they will need to be able to sync their agents' weights with the main agent.\n",
    "        This function loads the weights into this runner's agent.\n",
    "        \"\"\"\n",
    "        self.agent.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, gamma, agent, window = 15, workers = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.agent = agent\n",
    "        self.window = window\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=1e-4)\n",
    "        self.workers = workers\n",
    "\n",
    "        # even though we're loading the weights into worker agents explicitly, I found that still without sharing the weights as following, the algorithm was not converging:\n",
    "        self.agent.share_memory()\n",
    "\n",
    "    def fit(self, episodes = 1000):\n",
    "#            \"\"\"\n",
    "#            The higher level method for training the agents.\n",
    "#            It called into the lower level \"train\" which orchestrates the process itself.\n",
    "#            \"\"\"\n",
    "        last_update = 0\n",
    "        updates = dict()\n",
    "\n",
    "        for ix in range(1, self.workers + 1):\n",
    "            updates[ ix ] = { 'episode': 0, 'step': 0, 'rewards': deque(), 'losses': deque(), 'points': 0, 'mean_reward': 0, 'mean_loss': 0 }\n",
    "\n",
    "        for update in self.train(episodes):\n",
    "            now = time.time()\n",
    "\n",
    "            # you could do something useful here with the updates dict.\n",
    "            # I've opted out as I'm using logging anyways and got more value in just watching the log file, grepping for the desired values\n",
    "\n",
    "            # save the current model's weights every minute:\n",
    "            if now - last_update > 60:\n",
    "                torch.save(self.agent.state_dict(), './checkpoints/car-racing/' + str(int(now)) + '-.pytorch')\n",
    "                last_update = now\n",
    "\n",
    "    def train(self, episodes = 1000):\n",
    "        \"\"\"\n",
    "        Lower level training orchestration method. Written in the generator style. Intended to be used with \"for update in train(...):\"\n",
    "        \"\"\"\n",
    "\n",
    "        # create the requested number of background agents and runners:\n",
    "        worker_agents = self.agent.clone(num = self.workers)\n",
    "        runners = [ Runner(agent=agent, ix = ix + 1, train = True) for ix, agent in enumerate(worker_agents) ]\n",
    "\n",
    "        # we're going to communicate the workers' updates via the thread safe queue:\n",
    "        queue = mp.SimpleQueue()\n",
    "\n",
    "        # if we've not been given a number of episodes: assume the process is going to be interrupted with the keyboard interrupt once the user (us) decides so:\n",
    "        if episodes is None:\n",
    "            print('Starting out an infinite training process')\n",
    "\n",
    "        # create the actual background processes, making their entry be the train_one method:\n",
    "        processes = [ mp.Process(target=self.train_one, args=(runners[ix - 1], queue, episodes, ix)) for ix in range(1, self.workers + 1) ]\n",
    "\n",
    "        # run those processes:\n",
    "        for process in processes:\n",
    "            process.start()\n",
    "\n",
    "        try:\n",
    "            # what follows is a rather naive implementation of listening to workers updates. it works though for our purposes:\n",
    "            while any([ process.is_alive() for process in processes ]):\n",
    "                results = queue.get()\n",
    "                yield results\n",
    "        except Exception as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "    def train_one(self, runner, queue, episodes = 1000, ix = 1):\n",
    "        \"\"\"\n",
    "        Orchestrate the training for a single worker runner and agent. This is intended to run in its own background process.\n",
    "        \"\"\"\n",
    "\n",
    "        # possibly naive way of trying to de-correlate the weight updates further (I have no hard evidence to prove if it works, other than my subjective observation):\n",
    "        time.sleep(ix)\n",
    "\n",
    "        try:\n",
    "            # we are going to request the episode be reset whenever our agent scores lower than its max points. the same will happen if the agent scores total of -10 points:\n",
    "            max_points = 0\n",
    "            max_eval_points = 0\n",
    "            min_points = 0\n",
    "            max_episode = 0\n",
    "\n",
    "            for episode_ix in itertools.count(start=0, step=1):\n",
    "\n",
    "                if episodes is not None and episode_ix >= episodes:\n",
    "                    return\n",
    "\n",
    "                max_episode_points = 0\n",
    "                points = 0\n",
    "\n",
    "                # load up the newest weights every new episode:\n",
    "                runner.load_state_dict(self.agent.state_dict())\n",
    "\n",
    "                # every 5 episodes lets evaluate the weights we've learned so far by recording the run of the car using the greedy strategy:\n",
    "                if ix == 1 and episode_ix % 5 == 0:\n",
    "                    eval_points = self.record_greedy(episode_ix)\n",
    "\n",
    "                    if eval_points > max_eval_points:\n",
    "                        torch.save(runner.agent.state_dict(), './checkpoints/car-racing/' + str(eval_points) + '-eval-points.pytorch')\n",
    "                        max_eval_points = eval_points\n",
    "\n",
    "                # each n-step window, compute the gradients and apply\n",
    "                # also: decide if we shouldn't restart the episode if we don't want to explore too much of the not-useful state space:\n",
    "                for step, rewards, values, policies, action_ixs, terminal in runner.run_episode(yield_every=self.window):\n",
    "                    points += sum(rewards)\n",
    "\n",
    "                    if ix == 1 and points > max_points:\n",
    "                        torch.save(runner.agent.state_dict(), './checkpoints/car-racing/' + str(points) + '-points.pytorch')\n",
    "                        max_points = points\n",
    "\n",
    "                    if ix == 1 and episode_ix > max_episode:\n",
    "                        torch.save(runner.agent.state_dict(), './checkpoints/car-racing/' + str(episode_ix) + '-episode.pytorch')\n",
    "                        max_episode = episode_ix\n",
    "\n",
    "                    if points < -10 or (max_episode_points > min_points and points < min_points):\n",
    "                        terminal = True\n",
    "                        max_episode_points = 0\n",
    "                        point = 0\n",
    "                        runner.ask_reset()\n",
    "\n",
    "                    if terminal:\n",
    "                        logger.info('TERMINAL for ' + str(ix) + ' at step ' + str(step) + ' with total points ' + str(points) + ' max: ' + str(max_episode_points) )\n",
    "\n",
    "                    # if we're learning, then compute and apply the gradients and load the newest weights:\n",
    "                    if runner.train:\n",
    "                        loss = self.apply_gradients(policies, action_ixs, rewards, values, terminal, runner)\n",
    "                        runner.load_state_dict(self.agent.state_dict())\n",
    "\n",
    "                    max_episode_points = max(max_episode_points, points)\n",
    "                    min_points = max(min_points, points)\n",
    "\n",
    "                    # communicate the gathered values to the main process:\n",
    "                    queue.put((ix, episode_ix, step, rewards, loss, points, terminal))\n",
    "\n",
    "        except Exception as e:\n",
    "            string = traceback.format_exc()\n",
    "            logger.error(str(e) + ' → ' + string)\n",
    "            queue.put((ix, -1, -1, [-1], -1, str(e) + '<br />' + string, True))\n",
    "\n",
    "    def record_greedy(self, episode_ix):\n",
    "        \"\"\"\n",
    "        Records the video of the \"greedy\" run based on the current weights.\n",
    "        \"\"\"\n",
    "        directory = './videos/car-racing/episode-' + str(episode_ix) + '-' + str(int(time.time()))\n",
    "        player = Player(agent=self.agent, directory=directory, train=False)\n",
    "        points = player.play()\n",
    "        logger.info('Evaluation at episode ' + str(episode_ix) + ': ' + str(points) + ' points (' + directory + ')')\n",
    "        return points\n",
    "\n",
    "    def apply_gradients(self, policies, actions, rewards, values, terminal, runner):\n",
    "        worker_agent = runner.agent\n",
    "        actions_one_hot = torch.tensor([[ int(i == action) for i in range(4) ] for action in actions], dtype=torch.float32)\n",
    "\n",
    "        policies = torch.stack(policies)\n",
    "        values = torch.cat(values)\n",
    "        values_nograd = torch.zeros_like(values.detach(), requires_grad=False)\n",
    "        values_nograd.copy_(values)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(runner, rewards, values_nograd[-1], terminal)\n",
    "        advantages = discounted_rewards - values_nograd\n",
    "\n",
    "        logger.info('Runner ' + str(runner.ix) + 'Rewards: ' + str(rewards))\n",
    "        logger.info('Runner ' + str(runner.ix) + 'Discounted Rewards: ' + str(discounted_rewards.numpy()))\n",
    "\n",
    "        log_policies = torch.log(0.00000001 + policies)\n",
    "\n",
    "        one_log_policies = torch.sum(log_policies * actions_one_hot, dim=1)\n",
    "\n",
    "        entropy = torch.sum(policies * -log_policies)\n",
    "\n",
    "        policy_loss = -torch.mean(one_log_policies * advantages)\n",
    "\n",
    "        value_loss = F.mse_loss(values, discounted_rewards)\n",
    "\n",
    "        value_loss_nograd = torch.zeros_like(value_loss)\n",
    "        value_loss_nograd.copy_(value_loss)\n",
    "\n",
    "        policy_loss_nograd = torch.zeros_like(policy_loss)\n",
    "        policy_loss_nograd.copy_(policy_loss)\n",
    "\n",
    "        logger.info('Value Loss: ' + str(float(value_loss_nograd)) + ' Policy Loss: ' + str(float(policy_loss_nograd)))\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "        self.agent.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(worker_agent.parameters(), 40)\n",
    "\n",
    "        # the following step is crucial. at this point, all the info about the gradients reside in the worker agent's memory. We need to \"move\" those gradients into the main agent's memory:\n",
    "        self.share_gradients(worker_agent)\n",
    "\n",
    "        # update the weights with the computed gradients:\n",
    "        self.optimizer.step()\n",
    "\n",
    "        worker_agent.zero_grad()\n",
    "        return float(loss.detach())\n",
    "\n",
    "    def share_gradients(self, worker_agent):\n",
    "        for param, shared_param in zip(worker_agent.parameters(), self.agent.parameters()):\n",
    "            if shared_param.grad is not None:\n",
    "                return\n",
    "            shared_param._grad = param.grad\n",
    "\n",
    "    def clip_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Clips the rewards into the <-3, 3> range preventing too big of the gradients variance.\n",
    "        \"\"\"\n",
    "        return max(min(reward, 3), -3)\n",
    "\n",
    "    def discount_rewards(self, runner, rewards, last_value, terminal):\n",
    "        discounted_rewards = [0 for _ in rewards]\n",
    "        loop_rewards = [ self.clip_reward(reward) for reward in rewards ]\n",
    "\n",
    "        if terminal:\n",
    "            loop_rewards.append(0)\n",
    "        else:\n",
    "            loop_rewards.append(runner.get_value())\n",
    "\n",
    "        for main_ix in range(len(discounted_rewards) - 1, -1, -1):\n",
    "            for inside_ix in range(len(loop_rewards) - 1, -1, -1):\n",
    "                if inside_ix >= main_ix:\n",
    "                    reward = loop_rewards[inside_ix]\n",
    "                    discounted_rewards[main_ix] += self.gamma**(inside_ix - main_ix) * reward\n",
    "\n",
    "        return torch.tensor(discounted_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(Runner):\n",
    "    def __init__(self, directory, **kwargs):\n",
    "        super().__init__(ix=999, **kwargs)\n",
    "\n",
    "        self.env = Monitor(self.env, directory)\n",
    "\n",
    "    def play(self):\n",
    "        points = 0\n",
    "        for step, rewards, values, policies, actions, terminal in self.run_episode(yield_every = 1, do_render = True):\n",
    "            points += sum(rewards)\n",
    "        self.env.close()\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting out an infinite training process\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-391110e4753f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-21abcb687f6c>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mix\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'episode'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'step'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rewards'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'losses'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'points'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_reward'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-21abcb687f6c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# run those processes:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocess\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "\n",
    "    trainer = Trainer(gamma = 0.99, agent = agent)\n",
    "    trainer.fit(episodes=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
